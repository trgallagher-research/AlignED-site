<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Methodology - AlignED</title>
  <meta name="description" content="How AlignED benchmarks are constructed, scored, and validated. Four evaluation areas covering six benchmarks of professional teaching knowledge.">
  <link rel="stylesheet" href="../css/style.css">
  <!-- Mermaid.js for validation protocol diagram -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>
  <script>mermaid.initialize({ startOnLoad: true, theme: 'neutral' });</script>
</head>
<body>
  <header>
    <div class="container">
      <a href="../index.html" class="logo">AlignED</a>
      <button class="nav-toggle" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <nav>
        <a href="../index.html">Home</a>
        <a href="../about.html">About</a>
        <a href="../methodology/" class="active">Methodology</a>
        <a href="../results.html">Results</a>
        <a href="../benchmark-items.html">Benchmark Items</a>
        <a href="../data-access.html">Data & Contact</a>
      </nav>
    </div>
  </header>

  <main>
    <div class="page-header">
      <div class="container">
        <h1>Methodology</h1>
        <p class="subtitle">How we construct, administer, and score AlignED benchmarks</p>
      </div>
    </div>

    <section class="content-section">
      <div class="container">
        <div class="prose">
          <p>This section describes how each AlignED benchmark is constructed, validated, and scored. This is a living document updated as the methodology evolves. For a general overview, see <a href="../about.html">About</a>. To see the actual items used, visit <a href="../benchmark-items.html">Benchmark Items</a>.</p>

          <h2>Validation protocol</h2>
          <p>This is the validation framework AlignED is working towards. Not all tiers are complete for all benchmarks. Current status is noted below.</p>

          <div class="mermaid" style="margin: 2rem 0; text-align: center;">
graph LR
    A[Item Preparation] --> B[Model Administration]
    B --> C[Response Collection]
    C --> D[Scoring]
    D --> E[Tier 1: Baseline Reliability]
    E --> F[Tier 2: Robustness Probes]
    F --> G[Tier 3: Judge Validation]
    G --> H[Results Reported]

    style A fill:#EBF4FF,stroke:#3B6B9A
    style B fill:#EBF4FF,stroke:#3B6B9A
    style C fill:#EBF4FF,stroke:#3B6B9A
    style D fill:#EBF4FF,stroke:#3B6B9A
    style E fill:#FFF3E0,stroke:#D97706
    style F fill:#FFF3E0,stroke:#D97706
    style G fill:#FFF3E0,stroke:#D97706
    style H fill:#E8F5E9,stroke:#2F855A
          </div>

          <ul>
            <li><strong>Tier 1, Baseline Reliability:</strong> Multiple runs at T=0 to establish stable scores. <em>Status: Complete for neuromyths and scenarios. Partial for pedagogy and ACARA (single-run baselines reported).</em></li>
            <li><strong>Tier 2, Robustness Probes:</strong> Temperature variation (T=0, 0.5, 1.0), prompt sensitivity (4 framings), and confidence calibration probes. <em>Status: Complete for neuromyths. Not yet run for other benchmarks.</em></li>
            <li><strong>Tier 3, Judge Validation:</strong> For LLM-judged evaluations, a sample of scores is manually verified against the rubric. <em>Status: A sample of diagnostic reasoning scores has been manually verified. Systematic multi-judge validation is planned.</em></li>
          </ul>

          <h2>Benchmarks</h2>
          <p>AlignED covers four evaluation areas across six benchmarks. Each has its own methodology page with full details.</p>

          <table>
            <thead>
              <tr>
                <th>Evaluation area</th>
                <th>Benchmark</th>
                <th>Items</th>
                <th>Models</th>
                <th>Status</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><a href="neuromyths.html">Neuromyth Identification</a></td>
                <td>Myth/fact classification</td>
                <td>32</td>
                <td>31</td>
                <td>Complete</td>
              </tr>
              <tr>
                <td><a href="scenarios.html">Diagnostic Reasoning</a></td>
                <td>Implementation failure diagnosis</td>
                <td>12</td>
                <td>30</td>
                <td>Complete</td>
              </tr>
              <tr>
                <td rowspan="2"><a href="pedagogy.html">Teacher Certification Knowledge</a></td>
                <td>General pedagogy (CDPK)</td>
                <td>920</td>
                <td>23</td>
                <td>Complete</td>
              </tr>
              <tr>
                <td>Inclusive education (SEND)</td>
                <td>223</td>
                <td>23</td>
                <td>Complete</td>
              </tr>
              <tr>
                <td rowspan="2"><a href="acara.html">Student Work Judgement</a></td>
                <td>Comparative judgement (CJ)</td>
                <td>79 pairs</td>
                <td>12</td>
                <td>Complete</td>
              </tr>
              <tr>
                <td>Standards-based grading (SG)</td>
                <td>79 samples</td>
                <td>7</td>
                <td>Pilot</td>
              </tr>
            </tbody>
          </table>

          <h2>Evaluation dimensions</h2>
          <p>Beyond baseline accuracy, we assess multiple dimensions of model performance. These probes are currently complete for neuromyth identification only.</p>

          <ul class="benchmark-list">
            <li>
              <a href="dimensions.html#temperature">Temperature Robustness</a>
              <p class="description">Performance stability across temperature settings (T=0, 0.5, 1.0).</p>
            </li>
            <li>
              <a href="dimensions.html#prompt">Prompt Sensitivity</a>
              <p class="description">Consistency across prompt framings: standard, interrogative, embedded, adversarial.</p>
            </li>
            <li>
              <a href="dimensions.html#confidence">Confidence Calibration</a>
              <p class="description">Whether stated confidence tracks actual accuracy.</p>
            </li>
            <li>
              <a href="dimensions.html#efficiency">Token Efficiency</a>
              <p class="description">Performance relative to reasoning length and cost.</p>
            </li>
          </ul>

          <h2>Evaluation framework</h2>
          <p>AlignED follows a structured evaluation framework covering nine components. Each benchmark is audited against this framework. The table below shows the current status:</p>

          <table>
            <thead>
              <tr>
                <th>Component</th>
                <th>Status</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Item preparation</td>
                <td>Complete for all benchmarks</td>
              </tr>
              <tr>
                <td>Per-run configuration</td>
                <td>Complete for all benchmarks</td>
              </tr>
              <tr>
                <td>Validation (Tier 1)</td>
                <td>Partial (see above)</td>
              </tr>
              <tr>
                <td>Validation (Tier 2)</td>
                <td>Neuromyths only</td>
              </tr>
              <tr>
                <td>Validation (Tier 3)</td>
                <td>Sample-based for diagnostic reasoning</td>
              </tr>
              <tr>
                <td>Scoring pipeline</td>
                <td>Complete for all benchmarks</td>
              </tr>
              <tr>
                <td>Prompt templates</td>
                <td>Published on each methodology sub-page</td>
              </tr>
              <tr>
                <td>Shareable dataset</td>
                <td>In preparation (OSF)</td>
              </tr>
              <tr>
                <td>Human correlation studies</td>
                <td>Planned</td>
              </tr>
            </tbody>
          </table>

          <p>The full framework specification is available on <a href="https://github.com/trgallagher-research/AlignED-site" target="_blank">GitHub</a>.</p>

          <h2>Scoring and reporting</h2>
          <ul class="benchmark-list">
            <li>
              <a href="scoring.html">Scoring and Reporting</a>
              <p class="description">How each benchmark is scored, what model pools are used, and why results are reported separately rather than combined into a composite.</p>
            </li>
          </ul>

          <h2>Models evaluated</h2>
          <p>32 models from five providers have been tested across one or more benchmarks. Each benchmark has its own model pool (7&ndash;31 models).</p>

          <div class="grid grid-3 mt-3">
            <div class="card">
              <h4>Anthropic</h4>
              <p>Claude 3 through Claude 4.5 family, including extended thinking variants</p>
            </div>
            <div class="card">
              <h4>OpenAI</h4>
              <p>GPT-4 Turbo, GPT-4o, GPT-5 family, o3 and o4-mini</p>
            </div>
            <div class="card">
              <h4>Google, Meta, DeepSeek</h4>
              <p>Gemini 2.0&ndash;3, Llama 3.1&ndash;3.3, DeepSeek V3/R1</p>
            </div>
          </div>

          <h2>Reproducibility</h2>
          <p>All evaluation parameters are documented and fixed:</p>
          <ul>
            <li>Pinned model versions (specific API model strings)</li>
            <li>Fixed random seeds where applicable</li>
            <li>Standardised prompt templates (published on each methodology sub-page)</li>
            <li>Documented scoring rubrics with examples</li>
          </ul>

          <p>Raw data and scoring details are available through <a href="../data-access.html">Data & Contact</a>.</p>
        </div>
      </div>
    </section>
  </main>

  <footer>
    <div class="container">
      <div class="footer-content">
        <div class="footer-brand">
          <a href="../index.html" class="logo">AlignED</a>
          <p>Benchmarking AI performance on professional teaching tasks.</p>
        </div>
        <div class="footer-links">
          <h4>Navigation</h4>
          <ul>
            <li><a href="../about.html">About</a></li>
            <li><a href="../methodology/">Methodology</a></li>
            <li><a href="../results.html">Results</a></li>
            <li><a href="../benchmark-items.html">Benchmark Items</a></li>
            <li><a href="../data-access.html">Data & Contact</a></li>
          </ul>
        </div>
        <div class="footer-links">
          <h4>Resources</h4>
          <ul>
            <li><a href="https://github.com/trgallagher-research/AlignED-site" target="_blank">GitHub</a></li>
          </ul>
        </div>
      </div>
      <p class="copyright">&copy; 2026 AlignED. All rights reserved.</p>
      <p class="copyright">Last updated: February 2026</p>
    </div>
  </footer>

  <script src="../js/main.js"></script>
</body>
</html>
