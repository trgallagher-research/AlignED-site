<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Methodology - AlignED</title>
  <meta name="description" content="How AlignED benchmarks are constructed, scored, and validated. Transparent methodology for evaluating AI alignment with educational research.">
  <link rel="stylesheet" href="../css/style.css">
  <!-- Mermaid.js for validation protocol diagram -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.min.js"></script>
  <script>mermaid.initialize({ startOnLoad: true, theme: 'neutral' });</script>
</head>
<body>
  <header>
    <div class="container">
      <a href="../index.html" class="logo">AlignED</a>
      <button class="nav-toggle" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <nav>
        <a href="../index.html">Home</a>
        <a href="../about.html">About</a>
        <a href="../methodology/" class="active">Methodology</a>
        <a href="../results.html">Results</a>
        <a href="../benchmark-items.html">Benchmark Items</a>
        <a href="../data-access.html">Data</a>
        <a href="../contact.html">Contact</a>
      </nav>
    </div>
  </header>

  <main>
    <div class="page-header">
      <div class="container">
        <h1>Methodology</h1>
        <p class="subtitle">How we construct, administer, and score AlignED benchmarks</p>
      </div>
    </div>

    <section class="content-section">
      <div class="container">
        <div class="prose">
          <p>This section covers the technical details of how AlignED works. For a general overview, see <a href="../about.html">About</a>. To see the actual items used, visit <a href="../benchmark-items.html">Benchmark Items</a>.</p>

          <h2>What AlignED measures</h2>
          <p>AlignED reports two things: a composite index covering three knowledge benchmarks, and a separate evaluation of applied assessment judgement.</p>

          <p><strong>EAI Composite (21 models)</strong></p>

          <ul class="benchmark-list">
            <li>
              <a href="neuromyths.html">Educational Neuroscience (25%)</a>
              <p class="description">32 items testing identification of myths and facts about the brain and learning. 15 widely-believed myths + 17 verified facts.</p>
            </li>
            <li>
              <a href="scenarios.html">Classroom Reasoning (25%)</a>
              <p class="description">12 real-world scenarios scored 0&ndash;3 by an LLM judge (Claude 4.5 Sonnet). Models must diagnose why an evidence-based strategy is failing.</p>
            </li>
            <li>
              <a href="pedagogy.html">Pedagogical Knowledge (50%)</a>
              <p class="description">1,143 items from teacher certification assessments. General pedagogy (920 items, weighted 40%) and inclusive education (223 items, weighted 10%).</p>
            </li>
          </ul>

          <p>EAI = (0.25 &times; Neuroscience) + (0.25 &times; Scenarios) + (0.40 &times; CDPK) + (0.10 &times; SEND)</p>

          <p><strong>Reported separately</strong></p>

          <ul class="benchmark-list">
            <li>
              <a href="acara.html">Student Work Judgement / ACARA</a>
              <p class="description">79 verified pairs of student work compared against Australian curriculum standards. 12 models evaluated. Scored for accuracy and consistency. Reported separately because it tests applied assessment judgement rather than knowledge recall or reasoning. See <a href="acara.html">ACARA methodology</a> for details.</p>
            </li>
          </ul>

          <h2>Validation protocol</h2>
          <p>Each benchmark passes through a structured validation pipeline before results are reported:</p>

          <div class="mermaid" style="margin: 2rem 0; text-align: center;">
graph LR
    A[Item Preparation] --> B[Model Administration]
    B --> C[Response Collection]
    C --> D[Scoring]
    D --> E[Tier 1: Baseline Reliability]
    E --> F[Tier 2: Robustness Probes]
    F --> G[Tier 3: Judge Validation]
    G --> H[Composite Calculation]

    style A fill:#EBF4FF,stroke:#3B6B9A
    style B fill:#EBF4FF,stroke:#3B6B9A
    style C fill:#EBF4FF,stroke:#3B6B9A
    style D fill:#EBF4FF,stroke:#3B6B9A
    style E fill:#FFF3E0,stroke:#D97706
    style F fill:#FFF3E0,stroke:#D97706
    style G fill:#FFF3E0,stroke:#D97706
    style H fill:#E8F5E9,stroke:#2F855A
          </div>

          <ul>
            <li><strong>Tier 1, Baseline Reliability:</strong> Each model is run 5 times at T=0 to establish stable scores. Reported scores are means across iterations.</li>
            <li><strong>Tier 2, Robustness Probes:</strong> Temperature variation (T=0, 0.5, 1.0), prompt sensitivity (4 framings), and confidence calibration probes.</li>
            <li><strong>Tier 3, Judge Validation:</strong> For scenario scoring, a sample of LLM-judge scores is manually verified against the rubric.</li>
          </ul>

          <h2>Evaluation dimensions</h2>
          <p>Beyond baseline accuracy, we assess multiple dimensions of model performance.</p>

          <ul class="benchmark-list">
            <li>
              <a href="dimensions.html#temperature">Temperature Robustness</a>
              <p class="description">Performance stability across temperature settings (T=0, 0.5, 1.0).</p>
            </li>
            <li>
              <a href="dimensions.html#prompt">Prompt Sensitivity</a>
              <p class="description">Consistency across prompt framings: standard, interrogative, embedded, adversarial.</p>
            </li>
            <li>
              <a href="dimensions.html#confidence">Confidence Calibration</a>
              <p class="description">Whether stated confidence tracks actual accuracy.</p>
            </li>
            <li>
              <a href="dimensions.html#efficiency">Token Efficiency</a>
              <p class="description">Performance relative to reasoning length and cost.</p>
            </li>
          </ul>

          <h2>Composite scoring</h2>
          <ul class="benchmark-list">
            <li>
              <a href="scoring.html">Educational Alignment Index (EAI)</a>
              <p class="description">How we combine benchmark scores into a single composite metric. See <a href="scoring.html">EAI Scoring</a> for the full methodology.</p>
            </li>
          </ul>

          <h2>Models evaluated</h2>
          <p>21 models from three providers have complete EAI composite scores. 12 models (including 4 ACARA-only) were evaluated separately on student work judgement (ACARA).</p>

          <div class="grid grid-3 mt-3">
            <div class="card">
              <h4>Anthropic</h4>
              <p>Claude 3 through Claude 4.5 family, including extended thinking variants</p>
            </div>
            <div class="card">
              <h4>OpenAI</h4>
              <p>GPT-4 Turbo, GPT-4o, GPT-5 family, o3 and o4-mini</p>
            </div>
            <div class="card">
              <h4>Google</h4>
              <p>Gemini 2.0 Flash, 2.5 Pro, 3 Flash</p>
            </div>
          </div>

          <p class="mt-3">See the <a href="../results.html">Results page</a> for the full list of evaluated models and their scores.</p>

          <h2>Reproducibility</h2>
          <p>All evaluation parameters are documented and fixed:</p>
          <ul>
            <li>Pinned model versions (specific API model strings)</li>
            <li>Fixed random seeds where applicable</li>
            <li>Standardised prompt templates</li>
            <li>Documented scoring rubrics with examples</li>
          </ul>

          <p>Raw data and scoring details are available through our <a href="../data-access.html">Data Access</a> page.</p>
        </div>
      </div>
    </section>
  </main>

  <footer>
    <div class="container">
      <div class="footer-content">
        <div class="footer-brand">
          <a href="../index.html" class="logo">AlignED</a>
          <p>Transparent benchmarks for AI in education</p>
        </div>
        <div class="footer-links">
          <h4>Navigation</h4>
          <ul>
            <li><a href="../about.html">About</a></li>
            <li><a href="../methodology/">Methodology</a></li>
            <li><a href="../results.html">Results</a></li>
            <li><a href="../benchmark-items.html">Benchmark Items</a></li>
            <li><a href="../data-access.html">Data</a></li>
          </ul>
        </div>
        <div class="footer-links">
          <h4>Resources</h4>
          <ul>
            <li><a href="../contact.html">Contact</a></li>
            <li><a href="https://github.com/trgallagher-research/AlignED-site" target="_blank">GitHub</a></li>
          </ul>
        </div>
      </div>
      <p class="copyright">&copy; 2026 AlignED. All rights reserved.</p>
    </div>
  </footer>

  <script src="../js/main.js"></script>
</body>
</html>
