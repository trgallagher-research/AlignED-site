<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>About - AlignED</title>
  <meta name="description" content="What AlignED tests and the limits of what it can tell you. Four evaluation areas covering six benchmarks of AI performance on professional teaching tasks.">
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
  <header>
    <div class="container">
      <a href="index.html" class="logo">AlignED</a>
      <button class="nav-toggle" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <nav>
        <a href="index.html">Home</a>
        <a href="about.html" class="active">About</a>
        <a href="methodology/">Methodology</a>
        <a href="results.html">Results</a>
        <a href="benchmark-items.html">Benchmark Items</a>
        <a href="data-access.html">Data & Contact</a>
      </nav>
    </div>
  </header>

  <main>
    <div class="page-header">
      <div class="container">
        <h1>About AlignED</h1>
        <p class="subtitle">What we test and the limits of what it can tell you</p>
      </div>
    </div>

    <section class="content-section">
      <div class="container">
        <div class="prose">
          <h2>What AlignED is</h2>
          <p>There is no standard way to evaluate whether an AI model handles professional teaching tasks well. AlignED is building one.</p>
          <p>Some widely-believed claims about the brain and learning are wrong. About half of teachers endorse them (Dekker et al., 2012). If AI models repeat these same misconceptions, or give generic advice when a teacher needs a specific diagnosis, that is worth knowing before deploying them in schools.</p>
          <p>AlignED is an active research project. This site is a living document: results are updated as new models are tested and new evaluations are added.</p>

          <h2>What AlignED tests</h2>
          <p>Four evaluation areas cover six benchmarks related to professional teaching:</p>
          <ul>
            <li><strong>Neuromyth identification:</strong> 32 items testing identification of myths and facts about the brain and learning, adapted from Dekker et al. (2012). 31 models tested.</li>
            <li><strong>Diagnostic reasoning:</strong> 12 scenarios testing whether models can diagnose why evidence-based strategies fail when implemented incorrectly. 30 models tested.</li>
            <li><strong>Teacher certification knowledge:</strong> 1,143 items from Chilean teacher certification exams, reported separately for general pedagogy (920 items) and inclusive education (223 items). 23 models tested.</li>
            <li><strong>Student work judgement:</strong> 79 pairs of student work compared against Australian curriculum standards (ACARA), reported for both comparative judgement (12 models) and standards-based grading (7 models, pilot).</li>
          </ul>
          <p>Each benchmark has its own model pool and scoring method. Results are reported separately because performance on one does not predict performance on another. See <a href="methodology/">Methodology</a> for full details, <a href="methodology/scoring.html">Scoring and Reporting</a> for how each benchmark is scored, and <a href="benchmark-items.html">Benchmark Items</a> for the actual test content.</p>

          <h2>Current status</h2>
          <p>AlignED is a work in progress. Here is where things stand:</p>
          <ul>
            <li><strong>Completed:</strong> All six benchmarks have been administered to their respective model pools. Results are published on the <a href="results.html">Results</a> page.</li>
            <li><strong>In progress:</strong> A structured <a href="methodology/">evaluation framework</a> covering item preparation, validation protocol, and reliability metrics is being applied to each benchmark. Some tiers are complete for some benchmarks; others are planned.</li>
            <li><strong>Planned:</strong> Full dataset release on OSF. Human validation studies. Cross-cultural expansion of pedagogical knowledge items. New benchmark modules covering lesson planning quality and wellbeing/pastoral support.</li>
          </ul>

          <h2>Limitations</h2>
          <p>We want to be transparent about what this benchmark can and cannot tell you:</p>
          <ul>
            <li><strong>Training data contamination:</strong> Some benchmark items (especially the 32 neuromyths from Dekker et al., 2012) are published and may appear in model training data. High scores may partly reflect memorisation rather than robust knowledge.</li>
            <li><strong>LLM-as-judge scoring:</strong> The 12 diagnostic reasoning scenarios are scored by an LLM judge (Claude 4.5 Sonnet), which introduces potential judge model bias. A sample is validated manually, but systematic bias is possible.</li>
            <li><strong>Cultural specificity:</strong> The 1,143 pedagogical knowledge items are from Chilean teacher certification exams. While pedagogical principles are broadly universal, some items may reflect Chilean educational policy or context.</li>
            <li><strong>Varying sample sizes:</strong> The benchmarks range from 32 items (neuromyth identification) to 1,143 items (pedagogy), which affects statistical power differently across benchmarks.</li>
            <li><strong>Incomplete validation:</strong> The evaluation framework defines three validation tiers. Not all tiers are complete for all benchmarks. See <a href="methodology/">Methodology</a> for the current status of each tier.</li>
          </ul>

          <h2>What we claim and what we don't</h2>
          <p>AlignED measures how well AI models perform on specific benchmark tasks related to professional teaching. It does not measure whether a model is a good tutor, whether it can teach effectively, or whether it is safe to deploy in a classroom.</p>
          <p>A high score means the model answered these benchmark items correctly. It does not mean the model can apply that knowledge in a real classroom. These benchmarks are a starting point, not a finish line.</p>

          <div class="note">
            <p>A model that cannot identify common neuromyths or diagnose basic implementation failures may be a poor fit for educational applications. But a model that performs well on these benchmarks still needs to be evaluated for safety, bias, and pedagogical effectiveness before deployment.</p>
          </div>

          <h2>Planned work</h2>
          <p>These are research directions, not commitments to specific timelines:</p>
          <ul>
            <li>Tracking model capabilities over time as models improve and costs decrease</li>
            <li>New benchmark modules covering lesson planning quality, unit design, and wellbeing/pastoral support</li>
            <li>Human validation studies comparing model performance to teacher panels</li>
            <li>Cross-cultural expansion extending pedagogical knowledge items beyond the Chilean context</li>
            <li>Completing all three validation tiers for each benchmark (see <a href="methodology/">Methodology</a>)</li>
          </ul>

          <h2>Revision history</h2>
          <ul>
            <li><strong>February 2026:</strong> Site restructured. Composite EAI score removed in favour of per-benchmark reporting. ACARA standards-based grading pilot added. Model pool expanded to 32 models across five providers. Evaluation framework formalised.</li>
            <li><strong>January 2026:</strong> Initial public release with neuromyth identification, diagnostic reasoning, teacher certification knowledge, and ACARA comparative judgement benchmarks.</li>
          </ul>

          <div class="btn-group mt-4">
            <a href="methodology/" class="btn btn-primary">Explore Methodology</a>
            <a href="results.html" class="btn btn-secondary">View Results</a>
          </div>
        </div>
      </div>
    </section>
  </main>

  <footer>
    <div class="container">
      <div class="footer-content">
        <div class="footer-brand">
          <a href="index.html" class="logo">AlignED</a>
          <p>Benchmarking AI performance on professional teaching tasks.</p>
        </div>
        <div class="footer-links">
          <h4>Navigation</h4>
          <ul>
            <li><a href="about.html">About</a></li>
            <li><a href="methodology/">Methodology</a></li>
            <li><a href="results.html">Results</a></li>
            <li><a href="benchmark-items.html">Benchmark Items</a></li>
            <li><a href="data-access.html">Data & Contact</a></li>
          </ul>
        </div>
        <div class="footer-links">
          <h4>Resources</h4>
          <ul>
            <li><a href="https://github.com/trgallagher-research/AlignED-site" target="_blank">GitHub</a></li>
          </ul>
        </div>
      </div>
      <p class="copyright">&copy; 2026 AlignED. All rights reserved.</p>
      <p class="copyright">Last updated: February 2026</p>
    </div>
  </footer>

  <script src="js/main.js"></script>
</body>
</html>
