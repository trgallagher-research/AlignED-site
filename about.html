<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>About - AlignED</title>
  <meta name="description" content="Learn why evaluating AI alignment with educational research evidence matters for responsible AI deployment in education.">
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
  <header>
    <div class="container">
      <a href="index.html" class="logo">AlignED</a>
      <button class="nav-toggle" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <nav>
        <a href="index.html">Home</a>
        <a href="about.html" class="active">About</a>
        <a href="methodology/">Methodology</a>
        <a href="results.html">Results</a>
        <a href="benchmark-items.html">Benchmark Items</a>
        <a href="data-access.html">Data</a>
        <a href="contact.html">Contact</a>
      </nav>
    </div>
  </header>

  <main>
    <div class="page-header">
      <div class="container">
        <h1>About AlignED</h1>
        <p class="subtitle">What we test and the limits of what it can tell you</p>
      </div>
    </div>

    <section class="content-section">
      <div class="container">
        <div class="prose">
          <h2>The problem</h2>
          <p>When AI is used in educational settings, the quality of its responses depends partly on what it has absorbed about teaching and learning. Some widely-believed claims about the brain and learning are wrong. About half of teachers endorse them (Dekker et al., 2012). If AI models repeat these same misconceptions, or if they give generic advice when a teacher needs a specific diagnosis of why a strategy failed, that is worth knowing before deploying them in schools.</p>
          <p>There is no standard way to evaluate whether an AI model handles professional teaching tasks well. AlignED is building one.</p>

          <h2>Who this is for</h2>
          <ul>
            <li>Teachers and school leaders choosing which AI tools to use in classrooms</li>
            <li>Policymakers writing guidelines for AI in education</li>
            <li>EdTech companies selecting which model powers their tools</li>
            <li>Researchers looking for transparent, reproducible benchmarks</li>
          </ul>

          <h2>What AlignED tests</h2>
          <p>AlignED benchmarks AI across four areas related to professional teaching:</p>
          <ul>
            <li><strong>Educational neuroscience:</strong> 32 items testing identification of myths and facts about the brain and learning, adapted from Dekker et al. (2012).</li>
            <li><strong>Classroom reasoning:</strong> 12 scenarios testing diagnostic reasoning about why evidence-based strategies fail when implemented incorrectly.</li>
            <li><strong>Pedagogical knowledge:</strong> 1,143 teacher certification items covering general pedagogy (920 items) and inclusive education (223 items).</li>
            <li><strong>Student work judgement:</strong> 79 pairs of student work compared against Australian curriculum standards (ACARA).</li>
          </ul>
          <p>The Educational Alignment Index (EAI) composite combines the first three: 25% educational neuroscience + 25% classroom reasoning + 50% pedagogical knowledge. ACARA is reported separately because it tests a different capability (applied assessment judgement rather than knowledge recall or reasoning). See <a href="methodology/">Methodology</a> for full details and <a href="benchmark-items.html">Benchmark Items</a> for the actual test content.</p>

          <h2>Limitations</h2>
          <p>We want to be transparent about what this benchmark can and cannot tell you:</p>
          <ul>
            <li><strong>Training data contamination:</strong> Some benchmark items (especially the 32 neuromyths from Dekker et al., 2012) are published and may appear in model training data. High scores may partly reflect memorisation rather than robust knowledge.</li>
            <li><strong>LLM-as-judge scoring:</strong> The 12 classroom scenarios are scored by an LLM judge (Claude 4.5 Sonnet), which introduces potential judge model bias. We validate a sample manually, but systematic bias is possible.</li>
            <li><strong>Cultural specificity:</strong> The 1,143 pedagogical knowledge items are from Chilean teacher certification exams. While pedagogical principles are broadly universal, some items may reflect Chilean educational policy or context.</li>
            <li><strong>Varying sample sizes:</strong> The benchmarks range from 32 items (educational neuroscience) to 1,143 items (pedagogy), which affects statistical power differently across benchmarks.</li>
            <li><strong>Preliminary weighting:</strong> The composite score weights (25% educational neuroscience + 25% classroom reasoning + 50% pedagogical knowledge) are our initial best judgement, not empirically optimised.</li>
          </ul>

          <h2>What we claim and what we don't</h2>
          <p>AlignED measures how well AI models perform on specific benchmark tasks related to professional teaching. It does not measure whether a model is a good tutor, whether it can teach effectively, or whether it is safe to deploy in a classroom.</p>
          <p>A high score means the model answered these benchmark items correctly. It does not mean the model can apply that knowledge in a real classroom. These benchmarks are a starting point, not a finish line.</p>

          <div class="note">
            <p>AlignED tests performance on tasks related to professional teaching. A model that cannot identify common neuromyths or diagnose basic implementation failures may be a poor fit for educational applications. But a model that performs well on these benchmarks still needs to be evaluated for safety, bias, and pedagogical effectiveness before deployment.</p>
          </div>

          <h2>Planned work</h2>
          <p>AlignED is an ongoing project. Planned directions include:</p>
          <ul>
            <li>Tracking capabilities over time as models improve and costs decrease</li>
            <li>New benchmark modules covering lesson planning quality, unit design, and wellbeing/pastoral support</li>
            <li>Composite score refinement with empirically-grounded alternatives to the current weighting</li>
            <li>Human validation studies comparing model performance to teacher panels</li>
            <li>Cross-cultural expansion extending pedagogical knowledge items beyond the Chilean context</li>
          </ul>

          <div class="btn-group mt-4">
            <a href="methodology/" class="btn btn-primary">Explore Methodology</a>
            <a href="results.html" class="btn btn-secondary">View Results</a>
          </div>
        </div>
      </div>
    </section>
  </main>

  <footer>
    <div class="container">
      <div class="footer-content">
        <div class="footer-brand">
          <a href="index.html" class="logo">AlignED</a>
          <p>Transparent benchmarks for AI in education</p>
        </div>
        <div class="footer-links">
          <h4>Navigation</h4>
          <ul>
            <li><a href="about.html">About</a></li>
            <li><a href="methodology/">Methodology</a></li>
            <li><a href="results.html">Results</a></li>
            <li><a href="benchmark-items.html">Benchmark Items</a></li>
            <li><a href="data-access.html">Data</a></li>
          </ul>
        </div>
        <div class="footer-links">
          <h4>Resources</h4>
          <ul>
            <li><a href="contact.html">Contact</a></li>
            <li><a href="https://github.com/trgallagher-research/AlignED-site" target="_blank">GitHub</a></li>
          </ul>
        </div>
      </div>
      <p class="copyright">&copy; 2026 AlignED. All rights reserved.</p>
    </div>
  </footer>

  <script src="js/main.js"></script>
</body>
</html>
