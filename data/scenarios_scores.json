{
  "generated": "2026-02-12",
  "version": "v3.0",
  "description": "Diagnostic reasoning scenarios: 12 realistic classroom situations, scored 0-3 by LLM judge",
  "eval_name": "scenarios",
  "eval_display": "Diagnostic Reasoning",
  "n_items": 12,
  "n_models": 30,
  "judge_model": "Claude 4.5 Sonnet",
  "judge_model_id": "anthropic/claude-sonnet-4.5",
  "judge_provider": "OpenRouter",
  "scoring_notes": "356 of 360 scores from Sonnet judge; 4 Haiku fallback where Sonnet response was unparseable",
  "max_score": 36,
  "source": "AlignED/evals/scenarios/SCENARIO_PERFORMANCE_REPORT.md",
  "results": [
    { "model": "Claude 4.5 Sonnet", "provider": "Anthropic", "score_pct": 100.0, "raw_score": 36, "max_score": 36 },
    { "model": "GPT-5", "provider": "OpenAI", "score_pct": 100.0, "raw_score": 36, "max_score": 36 },
    { "model": "GPT-5.2", "provider": "OpenAI", "score_pct": 100.0, "raw_score": 36, "max_score": 36 },
    { "model": "Claude 3.7 Sonnet", "provider": "Anthropic", "score_pct": 97.2, "raw_score": 35, "max_score": 36 },
    { "model": "Claude 4 Opus", "provider": "Anthropic", "score_pct": 97.2, "raw_score": 35, "max_score": 36 },
    { "model": "Claude 4.1 Opus", "provider": "Anthropic", "score_pct": 97.2, "raw_score": 35, "max_score": 36 },
    { "model": "Claude 4.5 Opus", "provider": "Anthropic", "score_pct": 97.2, "raw_score": 35, "max_score": 36 },
    { "model": "Claude 4.5 Sonnet (Thinking)", "provider": "Anthropic", "score_pct": 97.2, "raw_score": 35, "max_score": 36 },
    { "model": "Claude 4 Sonnet", "provider": "Anthropic", "score_pct": 94.4, "raw_score": 34, "max_score": 36 },
    { "model": "Claude 4.5 Haiku", "provider": "Anthropic", "score_pct": 94.4, "raw_score": 34, "max_score": 36 },
    { "model": "Claude 4.5 Opus (Thinking)", "provider": "Anthropic", "score_pct": 94.4, "raw_score": 34, "max_score": 36 },
    { "model": "GPT-5 Mini", "provider": "OpenAI", "score_pct": 94.4, "raw_score": 34, "max_score": 36 },
    { "model": "o4-mini", "provider": "OpenAI", "score_pct": 94.4, "raw_score": 34, "max_score": 36 },
    { "model": "Claude 3.5 Sonnet", "provider": "Anthropic", "score_pct": 91.7, "raw_score": 33, "max_score": 36 },
    { "model": "o3", "provider": "OpenAI", "score_pct": 91.7, "raw_score": 33, "max_score": 36 },
    { "model": "Claude 3.5 Haiku", "provider": "Anthropic", "score_pct": 88.9, "raw_score": 32, "max_score": 36 },
    { "model": "Gemini 2.5 Pro", "provider": "Google", "score_pct": 88.9, "raw_score": 32, "max_score": 36 },
    { "model": "DeepSeek R1", "provider": "DeepSeek", "score_pct": 86.1, "raw_score": 31, "max_score": 36 },
    { "model": "Claude 3 Opus", "provider": "Anthropic", "score_pct": 83.3, "raw_score": 30, "max_score": 36 },
    { "model": "DeepSeek V3", "provider": "DeepSeek", "score_pct": 80.6, "raw_score": 29, "max_score": 36 },
    { "model": "GPT-4o", "provider": "OpenAI", "score_pct": 80.6, "raw_score": 29, "max_score": 36 },
    { "model": "Gemini 2.5 Flash", "provider": "Google", "score_pct": 77.8, "raw_score": 28, "max_score": 36 },
    { "model": "o3-mini", "provider": "OpenAI", "score_pct": 77.8, "raw_score": 28, "max_score": 36 },
    { "model": "GPT-4 Turbo", "provider": "OpenAI", "score_pct": 75.0, "raw_score": 27, "max_score": 36 },
    { "model": "Gemini 2.0 Flash", "provider": "Google", "score_pct": 75.0, "raw_score": 27, "max_score": 36 },
    { "model": "Llama 3.1 405B", "provider": "Meta", "score_pct": 75.0, "raw_score": 27, "max_score": 36 },
    { "model": "Claude 3 Haiku", "provider": "Anthropic", "score_pct": 69.4, "raw_score": 25, "max_score": 36 },
    { "model": "Llama 3.1 70B", "provider": "Meta", "score_pct": 69.4, "raw_score": 25, "max_score": 36 },
    { "model": "Llama 3.3 70B", "provider": "Meta", "score_pct": 66.7, "raw_score": 24, "max_score": 36 },
    { "model": "GPT-4o Mini", "provider": "OpenAI", "score_pct": 61.1, "raw_score": 22, "max_score": 36 }
  ]
}
